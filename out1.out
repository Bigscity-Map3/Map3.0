2024-03-17 06:26:37,595 - INFO - Log directory: ./libcity/log
2024-03-17 06:26:37,595 - INFO - Begin pipeline, task=region_representation, model_name=MVURE, dataset_name=bj_region, exp_id=73516
2024-03-17 06:26:37,595 - INFO - {'task': 'region_representation', 'model': 'MVURE', 'dataset': 'bj_region', 'saved_model': True, 'train': True, 'seed': 0, 'gpu': True, 'gpu_id': 3, 'embed_size': 128, 'task_epoch': 100, 'pre_len': 3, 'embed_epoch': 300, 'init_param': False, 'downstream_model': 'rnn', 'is_static': True, 'batch_size': 64, 'downstream_batch_size': 64, 'dataset_class': 'MVUREDataset', 'executor': 'TwoStepExecutor', 'evaluator': 'HHGCLEvaluator', 'output_dim': 12, 'representation_object': 'region', 'remove_node_type': 'MVURE', 'max_epoch': 10, 'geo': {'including_types': ['Point', 'LineString', 'Polygon'], 'Point': {'function': 'other', 'traffic_type': 'enum'}, 'LineString': {'function': 'other', 'traffic_type': 'enum'}, 'Polygon': {'function': 'num', 'traffic_type': 'enum'}}, 'rel': {'including_types': ['geo'], 'geo': {'rel_type': 'enum'}}, 'dyna': {'including_types': ['trajectory'], 'trajectory': {'geo_id': 'geo_id', 'total_traj_id': 'num'}}, 'data_files': 'bj_dataset', 'geo_file': 'bj_dataset', 'rel_file': 'bj_dataset', 'dyna_file': 'bj_dataset', 'calculate_weight_adj': True, 'device': device(type='cuda', index=3), 'exp_id': 73516}
2024-03-17 06:26:41,790 - INFO - finish construct flow graph
2024-03-17 06:26:42,930 - INFO - finish construct poi_simi
MVURE_Layer(
  (s_gat): GATConv(
    (fc): Linear(in_features=250, out_features=12, bias=False)
    (feat_drop): Dropout(p=0.0, inplace=False)
    (attn_drop): Dropout(p=0.2, inplace=False)
    (leaky_relu): LeakyReLU(negative_slope=0.2)
  )
  (t_gat): GATConv(
    (fc): Linear(in_features=250, out_features=12, bias=False)
    (feat_drop): Dropout(p=0.0, inplace=False)
    (attn_drop): Dropout(p=0.2, inplace=False)
    (leaky_relu): LeakyReLU(negative_slope=0.2)
  )
  (poi_gat): GATConv(
    (fc): Linear(in_features=250, out_features=12, bias=False)
    (feat_drop): Dropout(p=0.0, inplace=False)
    (attn_drop): Dropout(p=0.2, inplace=False)
    (leaky_relu): LeakyReLU(negative_slope=0.2)
  )
  (fused_layer): self_attn()
  (mv_layer): mv_attn(
    (sigmoid): Sigmoid()
  )
)
2024-03-17 06:26:46,620 - INFO - Total parameter numbers: 9108
2024-03-17 06:26:46,620 - INFO - start training,lr=0.005,weight_dacay=0.001
Using backend: pytorch
/home/zhangwt/miniconda3/envs/poi/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  return warnings.warn(message, category=category, stacklevel=1)
/home/zhangwt/remote/zwt/Map3.0/libcity/model/region_representation/MVURE.py:125: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  attn = F.softmax(attn)
Traceback (most recent call last):
  File "run_model.py", line 38, in <module>
    train=args.train, other_args=other_args)
  File "/home/zhangwt/remote/zwt/Map3.0/libcity/pipeline/pipeline.py", line 57, in run_model
    executor.train(train_data, valid_data)
  File "/home/zhangwt/remote/zwt/Map3.0/libcity/executor/twostep_executor.py", line 29, in train
    self.model.run()
  File "/home/zhangwt/remote/zwt/Map3.0/libcity/model/region_representation/MVURE.py", line 59, in run
    loss.backward()
  File "/home/zhangwt/miniconda3/envs/poi/lib/python3.7/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/zhangwt/miniconda3/envs/poi/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 5.62 GiB (GPU 3; 23.49 GiB total capacity; 13.30 GiB already allocated; 2.63 GiB free; 18.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
